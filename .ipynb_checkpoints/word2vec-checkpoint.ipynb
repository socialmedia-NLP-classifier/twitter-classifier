{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  4,  6, 12,  4,  6,  7, 12,  7,  6,  7,  8, 12,  7,  4,  7,\n",
       "         8,  6,  7,  4,  6,  8,  6,  0,  4,  6,  7,  6,  0,  2,  6,  7,\n",
       "         8,  0,  2,  7,  7,  8,  6,  2,  7,  1,  8,  6,  0,  7,  1,  5,\n",
       "         6,  0,  2,  1,  5, 10,  0,  2,  7,  5, 10, 11,  2,  7,  1, 10,\n",
       "        11,  3,  7,  1,  5, 11,  3,  9,  1,  5, 10,  3,  9,  5, 10, 11,\n",
       "         9, 10, 11,  3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.log(softmax_out[Y.flatten(), np.arange(Y.shape[1])] + 0.001))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    softmax_out[Y.flatten(), np.arange(m)] -= 1.0\n",
    "    dL_dZ = softmax_out\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\kukai\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.5521985351695675\n",
      "Cost after epoch 10: 2.551829980556583\n",
      "Cost after epoch 20: 2.5514419573439384\n",
      "Cost after epoch 30: 2.551010009137801\n",
      "Cost after epoch 40: 2.550509270633949\n",
      "Cost after epoch 50: 2.5499130245173816\n",
      "Cost after epoch 60: 2.5492061975714235\n",
      "Cost after epoch 70: 2.5483500820448266\n",
      "Cost after epoch 80: 2.547308938051144\n",
      "Cost after epoch 90: 2.546043830140218\n",
      "Cost after epoch 100: 2.544510594495772\n",
      "Cost after epoch 110: 2.5426954695118438\n",
      "Cost after epoch 120: 2.5405247059698457\n",
      "Cost after epoch 130: 2.5379352760225635\n",
      "Cost after epoch 140: 2.5348609733281346\n",
      "Cost after epoch 150: 2.5312293807996693\n",
      "Cost after epoch 160: 2.5270454630529957\n",
      "Cost after epoch 170: 2.5221831505882277\n",
      "Cost after epoch 180: 2.5165581255337823\n",
      "Cost after epoch 190: 2.510097916097571\n",
      "Cost after epoch 200: 2.5027399529041565\n",
      "Cost after epoch 210: 2.494596802217266\n",
      "Cost after epoch 220: 2.4855464703342878\n",
      "Cost after epoch 230: 2.4755905108392957\n",
      "Cost after epoch 240: 2.4647898792966303\n",
      "Cost after epoch 250: 2.453257073170984\n",
      "Cost after epoch 260: 2.441381422329118\n",
      "Cost after epoch 270: 2.429195708114072\n",
      "Cost after epoch 280: 2.4169238402725117\n",
      "Cost after epoch 290: 2.4048360031853093\n",
      "Cost after epoch 300: 2.393201195164525\n",
      "Cost after epoch 310: 2.382449900551667\n",
      "Cost after epoch 320: 2.3725636109948365\n",
      "Cost after epoch 330: 2.3636267748868236\n",
      "Cost after epoch 340: 2.3556690464963745\n",
      "Cost after epoch 350: 2.348643892470136\n",
      "Cost after epoch 360: 2.3425447385444995\n",
      "Cost after epoch 370: 2.3370952493300674\n",
      "Cost after epoch 380: 2.3321037927568393\n",
      "Cost after epoch 390: 2.3273818249623885\n",
      "Cost after epoch 400: 2.3227451886791615\n",
      "Cost after epoch 410: 2.3181117761795154\n",
      "Cost after epoch 420: 2.3132698920616828\n",
      "Cost after epoch 430: 2.308098449374872\n",
      "Cost after epoch 440: 2.302513015606857\n",
      "Cost after epoch 450: 2.296456657172783\n",
      "Cost after epoch 460: 2.290021308615979\n",
      "Cost after epoch 470: 2.2831105946295933\n",
      "Cost after epoch 480: 2.275725566779204\n",
      "Cost after epoch 490: 2.267896993627943\n",
      "Cost after epoch 500: 2.2596699349529494\n",
      "Cost after epoch 510: 2.251258004056854\n",
      "Cost after epoch 520: 2.242595945439967\n",
      "Cost after epoch 530: 2.233733616022435\n",
      "Cost after epoch 540: 2.224740413251747\n",
      "Cost after epoch 550: 2.2156857497533915\n",
      "Cost after epoch 560: 2.2067993234378975\n",
      "Cost after epoch 570: 2.197994924890173\n",
      "Cost after epoch 580: 2.189307018728326\n",
      "Cost after epoch 590: 2.1807814711647655\n",
      "Cost after epoch 600: 2.1724571003611963\n",
      "Cost after epoch 610: 2.1645097196933536\n",
      "Cost after epoch 620: 2.1568269266469997\n",
      "Cost after epoch 630: 2.1494111070442212\n",
      "Cost after epoch 640: 2.1422764033191726\n",
      "Cost after epoch 650: 2.1354327628763845\n",
      "Cost after epoch 660: 2.1290019839920933\n",
      "Cost after epoch 670: 2.122872891622729\n",
      "Cost after epoch 680: 2.1170321078535697\n",
      "Cost after epoch 690: 2.111477353065469\n",
      "Cost after epoch 700: 2.106204366094675\n",
      "Cost after epoch 710: 2.101295151720005\n",
      "Cost after epoch 720: 2.0966544278587222\n",
      "Cost after epoch 730: 2.092264253164229\n",
      "Cost after epoch 740: 2.0881163660882702\n",
      "Cost after epoch 750: 2.084202110235831\n",
      "Cost after epoch 760: 2.0805773946204846\n",
      "Cost after epoch 770: 2.0771677788919107\n",
      "Cost after epoch 780: 2.073957308068166\n",
      "Cost after epoch 790: 2.0709377944732124\n",
      "Cost after epoch 800: 2.068101230572984\n",
      "Cost after epoch 810: 2.065486463254027\n",
      "Cost after epoch 820: 2.06303826560928\n",
      "Cost after epoch 830: 2.060744154478109\n",
      "Cost after epoch 840: 2.0585973525503083\n",
      "Cost after epoch 850: 2.056591254965527\n",
      "Cost after epoch 860: 2.0547521789220102\n",
      "Cost after epoch 870: 2.0530400185772497\n",
      "Cost after epoch 880: 2.0514450876869605\n",
      "Cost after epoch 890: 2.049961737973163\n",
      "Cost after epoch 900: 2.048584461797767\n",
      "Cost after epoch 910: 2.0473302073852992\n",
      "Cost after epoch 920: 2.046170437628587\n",
      "Cost after epoch 930: 2.0450977138213933\n",
      "Cost after epoch 940: 2.044107424888854\n",
      "Cost after epoch 950: 2.043195120790488\n",
      "Cost after epoch 960: 2.0423711593056066\n",
      "Cost after epoch 970: 2.04161587413959\n",
      "Cost after epoch 980: 2.0409237784155354\n",
      "Cost after epoch 990: 2.0402913167421874\n",
      "Cost after epoch 1000: 2.0397150962721264\n",
      "Cost after epoch 1010: 2.0392010122163615\n",
      "Cost after epoch 1020: 2.038736057696618\n",
      "Cost after epoch 1030: 2.038316355950361\n",
      "Cost after epoch 1040: 2.037939284089143\n",
      "Cost after epoch 1050: 2.0376023512412336\n",
      "Cost after epoch 1060: 2.0373084062643367\n",
      "Cost after epoch 1070: 2.0370492755615834\n",
      "Cost after epoch 1080: 2.036822288138317\n",
      "Cost after epoch 1090: 2.0366255232835457\n",
      "Cost after epoch 1100: 2.0364571519870585\n",
      "Cost after epoch 1110: 2.036317897720004\n",
      "Cost after epoch 1120: 2.0362029959044627\n",
      "Cost after epoch 1130: 2.0361106148281127\n",
      "Cost after epoch 1140: 2.036039312742279\n",
      "Cost after epoch 1150: 2.035987707697129\n",
      "Cost after epoch 1160: 2.0359550517730445\n",
      "Cost after epoch 1170: 2.0359389379789623\n",
      "Cost after epoch 1180: 2.0359381047230944\n",
      "Cost after epoch 1190: 2.0359514338281324\n",
      "Cost after epoch 1200: 2.0359778482774575\n",
      "Cost after epoch 1210: 2.036015646049036\n",
      "Cost after epoch 1220: 2.036064020171164\n",
      "Cost after epoch 1230: 2.036122105842364\n",
      "Cost after epoch 1240: 2.0361890213837803\n",
      "Cost after epoch 1250: 2.0362639175702166\n",
      "Cost after epoch 1260: 2.036344563933726\n",
      "Cost after epoch 1270: 2.036431203997544\n",
      "Cost after epoch 1280: 2.0365232623684095\n",
      "Cost after epoch 1290: 2.036620049937858\n",
      "Cost after epoch 1300: 2.0367209063741174\n",
      "Cost after epoch 1310: 2.0368234128962297\n",
      "Cost after epoch 1320: 2.036928457686178\n",
      "Cost after epoch 1330: 2.037035684411474\n",
      "Cost after epoch 1340: 2.0371445723007473\n",
      "Cost after epoch 1350: 2.037254627480496\n",
      "Cost after epoch 1360: 2.0373634973963415\n",
      "Cost after epoch 1370: 2.0374724141064973\n",
      "Cost after epoch 1380: 2.0375811898361844\n",
      "Cost after epoch 1390: 2.037689454761173\n",
      "Cost after epoch 1400: 2.0377968644942097\n",
      "Cost after epoch 1410: 2.037901306453019\n",
      "Cost after epoch 1420: 2.038004139420856\n",
      "Cost after epoch 1430: 2.0381053058752783\n",
      "Cost after epoch 1440: 2.038204571471348\n",
      "Cost after epoch 1450: 2.0383017257799745\n",
      "Cost after epoch 1460: 2.0383949977368103\n",
      "Cost after epoch 1470: 2.0384857256078424\n",
      "Cost after epoch 1480: 2.0385739524706974\n",
      "Cost after epoch 1490: 2.0386595639766525\n",
      "Cost after epoch 1500: 2.0387424677928796\n",
      "Cost after epoch 1510: 2.038821273048153\n",
      "Cost after epoch 1520: 2.038897208616931\n",
      "Cost after epoch 1530: 2.0389703943562925\n",
      "Cost after epoch 1540: 2.0390408187935645\n",
      "Cost after epoch 1550: 2.0391084898403493\n",
      "Cost after epoch 1560: 2.0391723818921337\n",
      "Cost after epoch 1570: 2.0392335727636937\n",
      "Cost after epoch 1580: 2.0392922383507415\n",
      "Cost after epoch 1590: 2.0393484498229926\n",
      "Cost after epoch 1600: 2.0394022941288728\n",
      "Cost after epoch 1610: 2.03945305273289\n",
      "Cost after epoch 1620: 2.039501647126342\n",
      "Cost after epoch 1630: 2.039548289596035\n",
      "Cost after epoch 1640: 2.0395931101418303\n",
      "Cost after epoch 1650: 2.0396362500164\n",
      "Cost after epoch 1660: 2.039677208879601\n",
      "Cost after epoch 1670: 2.0397167729314636\n",
      "Cost after epoch 1680: 2.0397551717476685\n",
      "Cost after epoch 1690: 2.0397925679206517\n",
      "Cost after epoch 1700: 2.039829130248285\n",
      "Cost after epoch 1710: 2.03986447185332\n",
      "Cost after epoch 1720: 2.039899281715594\n",
      "Cost after epoch 1730: 2.0399337890302447\n",
      "Cost after epoch 1740: 2.039968162895275\n",
      "Cost after epoch 1750: 2.0400025736707357\n",
      "Cost after epoch 1760: 2.0400366428591385\n",
      "Cost after epoch 1770: 2.040071008924593\n",
      "Cost after epoch 1780: 2.0401058860975656\n",
      "Cost after epoch 1790: 2.040141427159647\n",
      "Cost after epoch 1800: 2.0401777819516336\n",
      "Cost after epoch 1810: 2.0402144887571283\n",
      "Cost after epoch 1820: 2.0402521833429206\n",
      "Cost after epoch 1830: 2.0402910549628217\n",
      "Cost after epoch 1840: 2.0403312231009605\n",
      "Cost after epoch 1850: 2.0403728012897795\n",
      "Cost after epoch 1860: 2.040415176495671\n",
      "Cost after epoch 1870: 2.0404590243162755\n",
      "Cost after epoch 1880: 2.040504504226315\n",
      "Cost after epoch 1890: 2.040551692589208\n",
      "Cost after epoch 1900: 2.0406006581786316\n",
      "Cost after epoch 1910: 2.0406505971648885\n",
      "Cost after epoch 1920: 2.0407022573642446\n",
      "Cost after epoch 1930: 2.04075576853663\n",
      "Cost after epoch 1940: 2.0408111612335285\n",
      "Cost after epoch 1950: 2.040868458100155\n",
      "Cost after epoch 1960: 2.0409266540708217\n",
      "Cost after epoch 1970: 2.0409865886006235\n",
      "Cost after epoch 1980: 2.0410483656860645\n",
      "Cost after epoch 1990: 2.0411119735458887\n",
      "Cost after epoch 2000: 2.0411773933080646\n",
      "Cost after epoch 2010: 2.0412434340042243\n",
      "Cost after epoch 2020: 2.0413110325816284\n",
      "Cost after epoch 2030: 2.0413802739352267\n",
      "Cost after epoch 2040: 2.041451112175226\n",
      "Cost after epoch 2050: 2.041523496003241\n",
      "Cost after epoch 2060: 2.041596083851316\n",
      "Cost after epoch 2070: 2.0416699038068438\n",
      "Cost after epoch 2080: 2.0417450298462803\n",
      "Cost after epoch 2090: 2.041821393254347\n",
      "Cost after epoch 2100: 2.041898922116843\n",
      "Cost after epoch 2110: 2.041976172129904\n",
      "Cost after epoch 2120: 2.0420542466072678\n",
      "Cost after epoch 2130: 2.0421332170175197\n",
      "Cost after epoch 2140: 2.0422130041926283\n",
      "Cost after epoch 2150: 2.0422935280836594\n",
      "Cost after epoch 2160: 2.042373293954602\n",
      "Cost after epoch 2170: 2.0424534580219165\n",
      "Cost after epoch 2180: 2.0425340964279237\n",
      "Cost after epoch 2190: 2.0426151308554132\n",
      "Cost after epoch 2200: 2.0426964840918433\n",
      "Cost after epoch 2210: 2.042776660074785\n",
      "Cost after epoch 2220: 2.0428568423050297\n",
      "Cost after epoch 2230: 2.042937116192927\n",
      "Cost after epoch 2240: 2.0430174125987866\n",
      "Cost after epoch 2250: 2.0430976648034505\n",
      "Cost after epoch 2260: 2.0431764155928422\n",
      "Cost after epoch 2270: 2.043254849662678\n",
      "Cost after epoch 2280: 2.043333062933883\n",
      "Cost after epoch 2290: 2.0434109997381262\n",
      "Cost after epoch 2300: 2.0434886073512835\n",
      "Cost after epoch 2310: 2.043564495885619\n",
      "Cost after epoch 2320: 2.043639827049185\n",
      "Cost after epoch 2330: 2.043714705328002\n",
      "Cost after epoch 2340: 2.0437890888988743\n",
      "Cost after epoch 2350: 2.0438629387212526\n",
      "Cost after epoch 2360: 2.04393494932685\n",
      "Cost after epoch 2370: 2.0440062388748204\n",
      "Cost after epoch 2380: 2.044076916258637\n",
      "Cost after epoch 2390: 2.0441469510259473\n",
      "Cost after epoch 2400: 2.044216314911055\n",
      "Cost after epoch 2410: 2.0442837950956756\n",
      "Cost after epoch 2420: 2.0443504500838503\n",
      "Cost after epoch 2430: 2.0444163881205037\n",
      "Cost after epoch 2440: 2.0444815862667145\n",
      "Cost after epoch 2450: 2.0445460230145476\n",
      "Cost after epoch 2460: 2.044608580984722\n",
      "Cost after epoch 2470: 2.0446702472893348\n",
      "Cost after epoch 2480: 2.044731124719202\n",
      "Cost after epoch 2490: 2.0447911939111787\n",
      "Cost after epoch 2500: 2.0448504362294733\n",
      "Cost after epoch 2510: 2.0449078302694095\n",
      "Cost after epoch 2520: 2.0449642835923365\n",
      "Cost after epoch 2530: 2.045019889685425\n",
      "Cost after epoch 2540: 2.045074629585761\n",
      "Cost after epoch 2550: 2.045128484524725\n",
      "Cost after epoch 2560: 2.045180529680295\n",
      "Cost after epoch 2570: 2.04523158735317\n",
      "Cost after epoch 2580: 2.0452817390779603\n",
      "Cost after epoch 2590: 2.0453309642343354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2600: 2.0453792420711316\n",
      "Cost after epoch 2610: 2.0454257463447707\n",
      "Cost after epoch 2620: 2.0454712097258736\n",
      "Cost after epoch 2630: 2.0455157002668662\n",
      "Cost after epoch 2640: 2.045559194748127\n",
      "Cost after epoch 2650: 2.045601669687023\n",
      "Cost after epoch 2660: 2.045642401251868\n",
      "Cost after epoch 2670: 2.0456820286038737\n",
      "Cost after epoch 2680: 2.045720605660913\n",
      "Cost after epoch 2690: 2.0457581065794503\n",
      "Cost after epoch 2700: 2.045794505273517\n",
      "Cost after epoch 2710: 2.0458291858323507\n",
      "Cost after epoch 2720: 2.0458626900185495\n",
      "Cost after epoch 2730: 2.0458950575631816\n",
      "Cost after epoch 2740: 2.04592626062324\n",
      "Cost after epoch 2750: 2.0459562712398065\n",
      "Cost after epoch 2760: 2.0459845880814065\n",
      "Cost after epoch 2770: 2.0460116514454465\n",
      "Cost after epoch 2780: 2.0460374871815974\n",
      "Cost after epoch 2790: 2.0460620664590072\n",
      "Cost after epoch 2800: 2.0460853605198657\n",
      "Cost after epoch 2810: 2.046106989726028\n",
      "Cost after epoch 2820: 2.04612728875371\n",
      "Cost after epoch 2830: 2.046146270052461\n",
      "Cost after epoch 2840: 2.0461639049818947\n",
      "Cost after epoch 2850: 2.046180165190028\n",
      "Cost after epoch 2860: 2.046194799579982\n",
      "Cost after epoch 2870: 2.0462080332105548\n",
      "Cost after epoch 2880: 2.046219865667737\n",
      "Cost after epoch 2890: 2.0462302696779378\n",
      "Cost after epoch 2900: 2.04623921847008\n",
      "Cost after epoch 2910: 2.046246595411696\n",
      "Cost after epoch 2920: 2.046252512431471\n",
      "Cost after epoch 2930: 2.0462569567740783\n",
      "Cost after epoch 2940: 2.0462599035884623\n",
      "Cost after epoch 2950: 2.046261328719237\n",
      "Cost after epoch 2960: 2.046261254455766\n",
      "Cost after epoch 2970: 2.0462596768977046\n",
      "Cost after epoch 2980: 2.046256571424763\n",
      "Cost after epoch 2990: 2.0462519164686754\n",
      "Cost after epoch 3000: 2.0462456913163156\n",
      "Cost after epoch 3010: 2.0462380597753755\n",
      "Cost after epoch 3020: 2.046228900520432\n",
      "Cost after epoch 3030: 2.04621817749144\n",
      "Cost after epoch 3040: 2.0462058730289145\n",
      "Cost after epoch 3050: 2.046191970447435\n",
      "Cost after epoch 3060: 2.046176775567311\n",
      "Cost after epoch 3070: 2.0461600493638574\n",
      "Cost after epoch 3080: 2.0461417447226866\n",
      "Cost after epoch 3090: 2.0461218482746775\n",
      "Cost after epoch 3100: 2.0461003477010222\n",
      "Cost after epoch 3110: 2.04607768913311\n",
      "Cost after epoch 3120: 2.046053517029688\n",
      "Cost after epoch 3130: 2.0460277735881296\n",
      "Cost after epoch 3140: 2.0460004498782065\n",
      "Cost after epoch 3150: 2.0459715380551304\n",
      "Cost after epoch 3160: 2.0459416207017216\n",
      "Cost after epoch 3170: 2.0459102284483577\n",
      "Cost after epoch 3180: 2.0458772931748563\n",
      "Cost after epoch 3190: 2.0458428103317745\n",
      "Cost after epoch 3200: 2.045806776453251\n",
      "Cost after epoch 3210: 2.045769904701985\n",
      "Cost after epoch 3220: 2.0457316160419383\n",
      "Cost after epoch 3230: 2.0456918324337012\n",
      "Cost after epoch 3240: 2.0456505534794887\n",
      "Cost after epoch 3250: 2.045607779832044\n",
      "Cost after epoch 3260: 2.045564347579277\n",
      "Cost after epoch 3270: 2.045519573553626\n",
      "Cost after epoch 3280: 2.0454733702418135\n",
      "Cost after epoch 3290: 2.045425741037453\n",
      "Cost after epoch 3300: 2.0453766903270223\n",
      "Cost after epoch 3310: 2.0453271679707443\n",
      "Cost after epoch 3320: 2.045276393422079\n",
      "Cost after epoch 3330: 2.0452242702002112\n",
      "Cost after epoch 3340: 2.0451708050389037\n",
      "Cost after epoch 3350: 2.0451160055892337\n",
      "Cost after epoch 3360: 2.045060925148789\n",
      "Cost after epoch 3370: 2.0450046935682398\n",
      "Cost after epoch 3380: 2.044947205974265\n",
      "Cost after epoch 3390: 2.0448884719359683\n",
      "Cost after epoch 3400: 2.044828501852695\n",
      "Cost after epoch 3410: 2.0447684412681593\n",
      "Cost after epoch 3420: 2.0447073390128314\n",
      "Cost after epoch 3430: 2.04464508246614\n",
      "Cost after epoch 3440: 2.0445816835069386\n",
      "Cost after epoch 3450: 2.0445171547513317\n",
      "Cost after epoch 3460: 2.0444527222591877\n",
      "Cost after epoch 3470: 2.0443873630033824\n",
      "Cost after epoch 3480: 2.0443209573275714\n",
      "Cost after epoch 3490: 2.044253518900931\n",
      "Cost after epoch 3500: 2.0441850620357305\n",
      "Cost after epoch 3510: 2.0441168813349955\n",
      "Cost after epoch 3520: 2.0440478914332423\n",
      "Cost after epoch 3530: 2.0439779664074322\n",
      "Cost after epoch 3540: 2.0439071212246587\n",
      "Cost after epoch 3550: 2.0438353714035036\n",
      "Cost after epoch 3560: 2.0437640681386116\n",
      "Cost after epoch 3570: 2.0436920733911665\n",
      "Cost after epoch 3580: 2.043619255786155\n",
      "Cost after epoch 3590: 2.043545631138074\n",
      "Cost after epoch 3600: 2.0434712157266186\n",
      "Cost after epoch 3610: 2.043397405635446\n",
      "Cost after epoch 3620: 2.0433230197720813\n",
      "Cost after epoch 3630: 2.0432479221558015\n",
      "Cost after epoch 3640: 2.0431721290493554\n",
      "Cost after epoch 3650: 2.043095657101604\n",
      "Cost after epoch 3660: 2.04301993601946\n",
      "Cost after epoch 3670: 2.0429437510643376\n",
      "Cost after epoch 3680: 2.0428669625125218\n",
      "Cost after epoch 3690: 2.0427895867320944\n",
      "Cost after epoch 3700: 2.042711640406483\n",
      "Cost after epoch 3710: 2.0426345761852924\n",
      "Cost after epoch 3720: 2.042557154739356\n",
      "Cost after epoch 3730: 2.042479233466084\n",
      "Cost after epoch 3740: 2.042400828553335\n",
      "Cost after epoch 3750: 2.042321956442328\n",
      "Cost after epoch 3760: 2.042244082740666\n",
      "Cost after epoch 3770: 2.0421659521198072\n",
      "Cost after epoch 3780: 2.042087419949589\n",
      "Cost after epoch 3790: 2.0420085020076955\n",
      "Cost after epoch 3800: 2.04192921427197\n",
      "Cost after epoch 3810: 2.041851026095347\n",
      "Cost after epoch 3820: 2.0417726741914044\n",
      "Cost after epoch 3830: 2.0416940127280543\n",
      "Cost after epoch 3840: 2.041615056894682\n",
      "Cost after epoch 3850: 2.0415358220360265\n",
      "Cost after epoch 3860: 2.0414577728554546\n",
      "Cost after epoch 3870: 2.0413796455401267\n",
      "Cost after epoch 3880: 2.041301293844999\n",
      "Cost after epoch 3890: 2.041222732239732\n",
      "Cost after epoch 3900: 2.0411439753122815\n",
      "Cost after epoch 3910: 2.041066475559034\n",
      "Cost after epoch 3920: 2.0409889754244337\n",
      "Cost after epoch 3930: 2.040911328993751\n",
      "Cost after epoch 3940: 2.040833549926312\n",
      "Cost after epoch 3950: 2.0407556519696404\n",
      "Cost after epoch 3960: 2.0406790686914036\n",
      "Cost after epoch 3970: 2.0406025549112248\n",
      "Cost after epoch 3980: 2.0405259657352866\n",
      "Cost after epoch 3990: 2.0404493139566378\n",
      "Cost after epoch 4000: 2.040372612432509\n",
      "Cost after epoch 4010: 2.0402972698968584\n",
      "Cost after epoch 4020: 2.0402220589950915\n",
      "Cost after epoch 4030: 2.0401468364872377\n",
      "Cost after epoch 4040: 2.0400716142727218\n",
      "Cost after epoch 4050: 2.0399964042963306\n",
      "Cost after epoch 4060: 2.0399225853380174\n",
      "Cost after epoch 4070: 2.0398489526642782\n",
      "Cost after epoch 4080: 2.0397753652611175\n",
      "Cost after epoch 4090: 2.039701834129863\n",
      "Cost after epoch 4100: 2.0396283703027134\n",
      "Cost after epoch 4110: 2.039556318228471\n",
      "Cost after epoch 4120: 2.0394844999572728\n",
      "Cost after epoch 4130: 2.0394127772109343\n",
      "Cost after epoch 4140: 2.039341160105868\n",
      "Cost after epoch 4150: 2.039269658778384\n",
      "Cost after epoch 4160: 2.0391995796642335\n",
      "Cost after epoch 4170: 2.039129775157402\n",
      "Cost after epoch 4180: 2.039060110162981\n",
      "Cost after epoch 4190: 2.0389905939386965\n",
      "Cost after epoch 4200: 2.0389212357539823\n",
      "Cost after epoch 4210: 2.0388533009932703\n",
      "Cost after epoch 4220: 2.0387856753881493\n",
      "Cost after epoch 4230: 2.038718227413196\n",
      "Cost after epoch 4240: 2.0386509655027725\n",
      "Cost after epoch 4250: 2.038583898096907\n",
      "Cost after epoch 4260: 2.0385182470808862\n",
      "Cost after epoch 4270: 2.0384529339905733\n",
      "Cost after epoch 4280: 2.0383878311970296\n",
      "Cost after epoch 4290: 2.038322946352383\n",
      "Cost after epoch 4300: 2.038258287109996\n",
      "Cost after epoch 4310: 2.0381950299455873\n",
      "Cost after epoch 4320: 2.038132134180004\n",
      "Cost after epoch 4330: 2.0380694763513634\n",
      "Cost after epoch 4340: 2.038007063373853\n",
      "Cost after epoch 4350: 2.0379449021596163\n",
      "Cost after epoch 4360: 2.0378841223501776\n",
      "Cost after epoch 4370: 2.037823722587643\n",
      "Cost after epoch 4380: 2.0377635837946517\n",
      "Cost after epoch 4390: 2.037703712192839\n",
      "Cost after epoch 4400: 2.037644113999315\n",
      "Cost after epoch 4410: 2.0375858710333405\n",
      "Cost after epoch 4420: 2.037528022390988\n",
      "Cost after epoch 4430: 2.0374704535474497\n",
      "Cost after epoch 4440: 2.0374131700767673\n",
      "Cost after epoch 4450: 2.0373561775464952\n",
      "Cost after epoch 4460: 2.0373005093558425\n",
      "Cost after epoch 4470: 2.0372452458234256\n",
      "Cost after epoch 4480: 2.037190277099472\n",
      "Cost after epoch 4490: 2.037135608153891\n",
      "Cost after epoch 4500: 2.0370812439484576\n",
      "Cost after epoch 4510: 2.037028169211873\n",
      "Cost after epoch 4520: 2.0369755059271997\n",
      "Cost after epoch 4530: 2.036923149000616\n",
      "Cost after epoch 4540: 2.036871102839182\n",
      "Cost after epoch 4550: 2.0368193718403598\n",
      "Cost after epoch 4560: 2.0367688921197202\n",
      "Cost after epoch 4570: 2.036718827475156\n",
      "Cost after epoch 4580: 2.036669077612099\n",
      "Cost after epoch 4590: 2.036619646413519\n",
      "Cost after epoch 4600: 2.0365705377514116\n",
      "Cost after epoch 4610: 2.0365226394573845\n",
      "Cost after epoch 4620: 2.036475157035659\n",
      "Cost after epoch 4630: 2.0364279950005115\n",
      "Cost after epoch 4640: 2.03638115674694\n",
      "Cost after epoch 4650: 2.036334645657648\n",
      "Cost after epoch 4660: 2.0362893018484534\n",
      "Cost after epoch 4670: 2.0362443721926584\n",
      "Cost after epoch 4680: 2.0361997659929885\n",
      "Cost after epoch 4690: 2.036155486190034\n",
      "Cost after epoch 4700: 2.0361115357108166\n",
      "Cost after epoch 4710: 2.036068707732838\n",
      "Cost after epoch 4720: 2.0360262899601267\n",
      "Cost after epoch 4730: 2.0359841964370293\n",
      "Cost after epoch 4740: 2.035942429680949\n",
      "Cost after epoch 4750: 2.035900992194509\n",
      "Cost after epoch 4760: 2.0358606311767202\n",
      "Cost after epoch 4770: 2.0358206744481753\n",
      "Cost after epoch 4780: 2.035781040724918\n",
      "Cost after epoch 4790: 2.035741732130332\n",
      "Cost after epoch 4800: 2.035702750771897\n",
      "Cost after epoch 4810: 2.0356647989878396\n",
      "Cost after epoch 4820: 2.035627243848459\n",
      "Cost after epoch 4830: 2.0355900086515906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 4840: 2.0355530951540257\n",
      "Cost after epoch 4850: 2.0355165050956647\n",
      "Cost after epoch 4860: 2.03548089720723\n",
      "Cost after epoch 4870: 2.035445676810302\n",
      "Cost after epoch 4880: 2.0354107716774843\n",
      "Cost after epoch 4890: 2.035376183224999\n",
      "Cost after epoch 4900: 2.035341912851348\n",
      "Cost after epoch 4910: 2.0353085770481907\n",
      "Cost after epoch 4920: 2.0352756182777263\n",
      "Cost after epoch 4930: 2.035242968665824\n",
      "Cost after epoch 4940: 2.03521062931304\n",
      "Cost after epoch 4950: 2.0351786013015856\n",
      "Cost after epoch 4960: 2.0351474603488495\n",
      "Cost after epoch 4970: 2.0351166848524973\n",
      "Cost after epoch 4980: 2.0350862111580863\n",
      "Cost after epoch 4990: 2.03505604007454\n",
      "training time: 0:00:01.736716\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investing's neighbor words: ['the', 'stock', 'beating', 'costs']\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'beating']\n",
      "beating's neighbor words: ['market', 'stock', 'costs', 'investing']\n",
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
